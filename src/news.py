import feedparser
from typing import List, Dict, Optional
from datetime import datetime, timedelta
from urllib.parse import quote_plus
import json
import time
import re
import requests
from bs4 import BeautifulSoup
from src.utils.logger import setup_logger
from pathlib import Path

logger = setup_logger('news')

class News:
    """ì½”ì¸ ê´€ë ¨ ë‰´ìŠ¤ ìˆ˜ì§‘ê¸°"""
    
    # ë‰´ìŠ¤ ì†ŒìŠ¤ URL
    GOOGLE_NEWS_RSS = "https://news.google.com/rss/search?q={query}&hl=ko&gl=KR&ceid=KR:ko"
    NAVER_NEWS_SEARCH = "https://search.naver.com/search.naver?where=news&query={query}"
    COINDESK_RSS = "https://www.coindesk.com/arc/outboundfeeds/rss/?outputType=xml"  # ê¸°ë³¸ RSS
    COINDESK_SEARCH = "https://www.coindesk.com/search?q={query}"  # ê²€ìƒ‰ìš©
    COINTELEGRAPH_RSS = "https://cointelegraph.com/rss"  # ê¸°ë³¸ RSS
    COINTELEGRAPH_SEARCH = "https://cointelegraph.com/search?query={query}"  # ê²€ìƒ‰ìš©
    
    # ì‹¬ë³¼ë³„ ì¶”ê°€ ê²€ìƒ‰ í‚¤ì›Œë“œ
    SYMBOL_KEYWORDS = {
        "BTC": ["ë¹„íŠ¸ì½”ì¸", "Bitcoin", "BTC", "$BTC", "bitcoin"],
        "ETH": ["ì´ë”ë¦¬ì›€", "Ethereum", "ETH", "$ETH", "ethereum"],
        "XRP": ["ë¦¬í”Œ", "Ripple", "XRP", "$XRP", "ripple"],
        "DOGE": ["ë„ì§€ì½”ì¸", "Dogecoin", "DOGE", "$DOGE", "dogecoin"],
        "SOL": ["ì†”ë¼ë‚˜", "Solana", "SOL", "$SOL", "solana"],
    }
    
    # ê¸°ë³¸ ê²€ìƒ‰ í‚¤ì›Œë“œ (ëª¨ë“  ì‹¬ë³¼ì— ê³µí†µ ì ìš©)
    COMMON_KEYWORDS = [
        "ê°€ìƒìì‚°", "ì•”í˜¸í™”í", "í¬ë¦½í† ",
        "SEC", "CFTC", "ì½”ì¸ë² ì´ìŠ¤", "ë°”ì´ë‚¸ìŠ¤",
        "cryptocurrency", "crypto", "blockchain"
    ]
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        self.last_update = None
        self.cached_news = {}  # symbolë³„ ìºì‹œ
        self.cache_duration = 300  # 5ë¶„ ìºì‹œ
        
        # ë¡œê·¸ ë””ë ‰í† ë¦¬ ì„¤ì •
        self._setup_log_directory()
    
    def _setup_log_directory(self):
        """ë¡œê·¸ ë””ë ‰í† ë¦¬ ì„¤ì •"""
        # ê¸°ë³¸ ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
        base_dir = Path(".temp")
        base_dir.mkdir(exist_ok=True)
        
        # ì‹¤í–‰ ì‹œê°„ ê¸°ë°˜ ë””ë ‰í† ë¦¬ ìƒì„±
        now = datetime.now()
        run_id = now.strftime("%Y%m%d_%H%M%S")
        self.run_dir = base_dir / run_id
        self.run_dir.mkdir(exist_ok=True)
        
        # ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
        self.log_dir = self.run_dir / "logs"
        self.log_dir.mkdir(exist_ok=True)
    
    def _convert_datetime(self, data: Dict) -> Dict:
        """datetime ê°ì²´ë¥¼ ISO í˜•ì‹ ë¬¸ìì—´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
        if isinstance(data, dict):
            return {k: self._convert_datetime(v) for k, v in data.items()}
        elif isinstance(data, list):
            return [self._convert_datetime(item) for item in data]
        elif isinstance(data, datetime):
            return data.isoformat()
        return data
    
    def _save_news_collection(self, symbol: str, data: Dict, category: str):
        """ë‰´ìŠ¤ ìˆ˜ì§‘ ë°ì´í„°ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
        
        Args:
            symbol: ì‹¬ë³¼ (ì˜ˆ: BTC)
            data: ì €ì¥í•  ë°ì´í„°
            category: ì €ì¥ ì¹´í…Œê³ ë¦¬ (news_collection/news_cache)
        """
        categories = {
            'news_collection': '01_news_collection',
            'news_cache': '02_news_cache'
        }
        
        if category not in categories:
            logger.error(f"ì˜ëª»ëœ ì¹´í…Œê³ ë¦¬ì…ë‹ˆë‹¤: {category}")
            return
            
        timestamp = datetime.now().strftime("%H%M%S")
        filename = f"{categories[category]}_{symbol}_{timestamp}.json"
        filepath = self.log_dir / filename
        
        # datetime ê°ì²´ ë³€í™˜
        data = self._convert_datetime(data)
        
        # ë°ì´í„° í¬ë§·íŒ…
        formatted_data = {
            "timestamp": datetime.now().isoformat(),
            "symbol": symbol,
            "data_type": category,
            "content": data
        }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(formatted_data, f, ensure_ascii=False, indent=2)
            
        logger.info(f"{symbol} {category} ì €ì¥ ì™„ë£Œ: {filepath}")
    
    def _parse_datetime(self, date_str: str) -> datetime:
        """ë‰´ìŠ¤ ë°œí–‰ì¼ìë¥¼ íŒŒì‹±í•©ë‹ˆë‹¤."""
        try:
            # RSS í‘œì¤€ í˜•ì‹
            return datetime.strptime(date_str, "%a, %d %b %Y %H:%M:%S %Z")
        except:
            try:
                # ë„¤ì´ë²„ ë‰´ìŠ¤ í˜•ì‹
                if "ë¶„ ì „" in date_str:
                    minutes = int(date_str.replace("ë¶„ ì „", ""))
                    return datetime.now() - timedelta(minutes=minutes)
                elif "ì‹œê°„ ì „" in date_str:
                    hours = int(date_str.replace("ì‹œê°„ ì „", ""))
                    return datetime.now() - timedelta(hours=hours)
                elif "ì¼ ì „" in date_str:
                    days = int(date_str.replace("ì¼ ì „", ""))
                    return datetime.now() - timedelta(days=days)
                elif re.match(r'\d{4}\.\d{2}\.\d{2}\.', date_str):
                    # YYYY.MM.DD. í˜•ì‹
                    return datetime.strptime(date_str.strip('.'), "%Y.%m.%d")
                elif re.match(r'\d{4}\-\d{2}\-\d{2}', date_str):
                    # YYYY-MM-DD í˜•ì‹
                    return datetime.strptime(date_str.split()[0], "%Y-%m-%d")
                else:
                    logger.debug(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ë‚ ì§œ í˜•ì‹: {date_str}")
                    return datetime.now()
            except Exception as e:
                logger.debug(f"ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨: {date_str} (ì—ëŸ¬: {str(e)})")
                return datetime.now()
    
    def _clean_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ë¥¼ ì •ì œí•©ë‹ˆë‹¤."""
        # HTML íƒœê·¸ ë° ì—”í‹°í‹° ì œê±°
        text = re.sub(r'<[^>]+>', '', text)
        text = re.sub(r'&[a-zA-Z]+;', ' ', text)
        # íŠ¹ìˆ˜ë¬¸ì ì •ì œ (í•œê¸€, ì˜ë¬¸, ìˆ«ì, ê¸°ë³¸ ë¬¸ì¥ë¶€í˜¸ë§Œ í—ˆìš©)
        text = re.sub(r'[^\w\sê°€-í£.,Â·\-()%]', '', text)
        # ì—°ì†ëœ ê³µë°± ì œê±°
        text = re.sub(r'\s+', ' ', text)
        return text.strip()
    
    def _get_symbol_keywords(self, symbol: str) -> List[str]:
        """ì‹¬ë³¼ì— ëŒ€í•œ ê²€ìƒ‰ í‚¤ì›Œë“œ ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
        symbol = symbol.upper()
        keywords = self.SYMBOL_KEYWORDS.get(symbol, [symbol])
        if symbol not in self.SYMBOL_KEYWORDS:
            # ê¸°ë³¸ í‚¤ì›Œë“œ ìƒì„±
            keywords.extend([
                f"{symbol} ì½”ì¸",
                f"{symbol} ì‹œì„¸",
                f"{symbol} ê°€ê²©"
            ])
        return keywords
    
    def _get_coindesk_news(self, keyword: str, max_age_hours: int = 24) -> List[Dict]:
        """CoinDesk ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤."""
        news_items = []
        now = datetime.now()
        
        try:
            # RSS í”¼ë“œì—ì„œ ìµœì‹  ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
            feed = feedparser.parse(self.COINDESK_RSS)
            
            for entry in feed.entries:
                # í‚¤ì›Œë“œ í•„í„°ë§
                if not any(kw.lower() in entry.title.lower() or 
                          kw.lower() in entry.description.lower() 
                          for kw in [keyword]):
                    continue
                
                published_at = self._parse_datetime(entry.published)
                age_hours = (now - published_at).total_seconds() / 3600
                
                if age_hours > max_age_hours:
                    continue
                
                # ì œëª©ê³¼ ì¶œì²˜ ì •ì œ
                clean_title = self._clean_text(entry.title)
                
                # ìš”ì•½ ì •ì œ
                summary = self._clean_text(entry.description)
                if clean_title in summary:
                    summary = summary.replace(clean_title, "").strip()
                
                news_items.append({
                    "title": clean_title,
                    "summary": summary,
                    "published_at": published_at,
                    "source": "CoinDesk"
                })
            
            # ê²€ìƒ‰ APIë¥¼ í†µí•œ ì¶”ê°€ ë‰´ìŠ¤ ìˆ˜ì§‘
            search_url = self.COINDESK_SEARCH.format(query=quote_plus(keyword))
            response = self.session.get(search_url)
            soup = BeautifulSoup(response.text, 'html.parser')
            
            for article in soup.select('article'):
                title_elem = article.select_one('h6')
                if not title_elem:
                    continue
                
                title = self._clean_text(title_elem.text)
                
                time_elem = article.select_one('time')
                published = time_elem.get('datetime') if time_elem else None
                published_at = self._parse_datetime(published) if published else now
                
                age_hours = (now - published_at).total_seconds() / 3600
                if age_hours > max_age_hours:
                    continue
                
                summary = ""
                desc_elem = article.select_one('p')
                if desc_elem:
                    summary = self._clean_text(desc_elem.text)
                
                news_items.append({
                    "title": title,
                    "summary": summary,
                    "published_at": published_at,
                    "source": "CoinDesk"
                })
            
            logger.debug(f"CoinDesk ë‰´ìŠ¤ {len(news_items)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"CoinDesk ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}")
        
        return news_items

    def _collect_cointelegraph_news(self, keyword: str, max_age_hours: int) -> List[Dict]:
        """Cointelegraph ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤."""
        news_items = []
        now = datetime.now()
        
        try:
            # RSS í”¼ë“œì—ì„œ ìµœì‹  ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
            feed = feedparser.parse(self.COINTELEGRAPH_RSS)
            
            for entry in feed.entries:
                # í‚¤ì›Œë“œ í•„í„°ë§
                if not any(kw.lower() in entry.title.lower() or 
                          kw.lower() in entry.description.lower() 
                          for kw in [keyword]):
                    continue
                
                published_at = self._parse_datetime(entry.published)
                age_hours = (now - published_at).total_seconds() / 3600
                
                if age_hours > max_age_hours:
                    continue
                
                # ì œëª©ê³¼ ì¶œì²˜ ì •ì œ
                clean_title = self._clean_text(entry.title)
                
                # ìš”ì•½ ì •ì œ
                summary = self._clean_text(entry.description)
                if clean_title in summary:
                    summary = summary.replace(clean_title, "").strip()
                
                news_items.append({
                    "title": clean_title,
                    "summary": summary,
                    "published_at": published_at,
                    "source": "Cointelegraph"
                })
            
            # ê²€ìƒ‰ APIë¥¼ í†µí•œ ì¶”ê°€ ë‰´ìŠ¤ ìˆ˜ì§‘
            search_url = self.COINTELEGRAPH_SEARCH.format(query=quote_plus(keyword))
            response = self.session.get(search_url)
            soup = BeautifulSoup(response.text, 'html.parser')
            
            for article in soup.select('article.post-card'):
                title_elem = article.select_one('.post-card__title')
                if not title_elem:
                    continue
                
                title = self._clean_text(title_elem.text)
                
                time_elem = article.select_one('time')
                published = time_elem.get('datetime') if time_elem else None
                published_at = self._parse_datetime(published) if published else now
                
                age_hours = (now - published_at).total_seconds() / 3600
                if age_hours > max_age_hours:
                    continue
                
                summary = ""
                desc_elem = article.select_one('.post-card__text')
                if desc_elem:
                    summary = self._clean_text(desc_elem.text)
                
                news_items.append({
                    "title": title,
                    "summary": summary,
                    "published_at": published_at,
                    "source": "Cointelegraph"
                })
            
            logger.debug(f"Cointelegraph ë‰´ìŠ¤ {len(news_items)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"Cointelegraph ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}")
        
        return news_items

    def get_news(
        self,
        symbol: str,
        max_age_hours: int = 24,
        limit: int = 10,
        use_cache: bool = False
    ) -> List[Dict]:
        """íŠ¹ì • ì‹¬ë³¼ì˜ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
        
        Args:
            symbol: ì‹¬ë³¼ (ì˜ˆ: BTC)
            max_age_hours: ìµœëŒ€ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œê°„ (ì‹œê°„)
            limit: ìˆ˜ì§‘í•  ë‰´ìŠ¤ ê°œìˆ˜
            use_cache: ìºì‹œ ì‚¬ìš© ì—¬ë¶€
            
        Returns:
            List[Dict]: ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ëª©ë¡
        """
        symbol = symbol.upper()
        now = datetime.now()
        
        # ìºì‹œ ì²´í¬
        if (
            use_cache
            and symbol in self.cached_news
            and self.last_update is not None
            and (now - self.last_update).total_seconds() < self.cache_duration
        ):
            logger.debug(f"{symbol} ìºì‹œëœ ë‰´ìŠ¤ ë°˜í™˜")
            cached_data = {
                "source": "cache",
                "news_items": self.cached_news[symbol]
            }
            self._save_news_collection(symbol, cached_data, "news_cache")
            return self.cached_news[symbol]
        
        all_news = []
        keywords = self._get_symbol_keywords(symbol)
        
        # í‚¤ì›Œë“œë³„ ë‰´ìŠ¤ ìˆ˜ì§‘
        for keyword in keywords:
            try:
                # êµ¬ê¸€ ë‰´ìŠ¤ ìˆ˜ì§‘
                google_news = self._collect_google_news(keyword, max_age_hours)
                all_news.extend(google_news)
                
                # ë„¤ì´ë²„ ë‰´ìŠ¤ ìˆ˜ì§‘
                naver_news = self._collect_naver_news(keyword, max_age_hours)
                all_news.extend(naver_news)
                
                # CoinDesk ë‰´ìŠ¤ ìˆ˜ì§‘
                coindesk_news = self._get_coindesk_news(keyword, max_age_hours)
                all_news.extend(coindesk_news)
                
                # Cointelegraph ë‰´ìŠ¤ ìˆ˜ì§‘
                cointelegraph_news = self._collect_cointelegraph_news(keyword, max_age_hours)
                all_news.extend(cointelegraph_news)
                
                time.sleep(1)  # API í˜¸ì¶œ ê°„ê²© ì¡°ì ˆ
                
            except Exception as e:
                logger.error(f"{symbol} {keyword} ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}")
                continue
                
        # ì¤‘ë³µ ì œê±° (ì œëª© ê¸°ì¤€)
        unique_news = list({news["title"]: news for news in all_news}.values())
        
        # ë°œí–‰ì¼ì‹œ ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ í›„ limit ì ìš©
        unique_news.sort(key=lambda x: x["published_at"], reverse=True)
        news_list = unique_news[:limit]
        
        # ìˆ˜ì§‘ ê²°ê³¼ ì €ì¥
        self._save_news_collection(symbol, news_list, "news_collection")
        
        # ìºì‹œ ì—…ë°ì´íŠ¸
        if use_cache:
            self.cached_news[symbol] = news_list
            self.last_update = now
            cache_data = {
                "source": "cache",
                "news_items": news_list
            }
            self._save_news_collection(symbol, cache_data, "news_cache")
        
        logger.info(f"{symbol} ë‰´ìŠ¤ {len(news_list)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
        return news_list
    
    def format_news(
        self,
        news_items: List[Dict],
        show_summary: bool = True
    ) -> str:
        """ë‰´ìŠ¤ ê²°ê³¼ë¥¼ ë³´ê¸° ì¢‹ê²Œ í¬ë§·íŒ…í•©ë‹ˆë‹¤."""
        if not news_items:
            return "ìˆ˜ì§‘ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤."
        
        current_time = datetime.now().strftime("%Y-%m-%d %H:%M")
        
        output = []
        output.append(f"\nğŸ“° ë‰´ìŠ¤ ëª¨ë‹ˆí„°ë§ ({current_time})")
        output.append("=" * 60)
        
        # í†µê³„ ì •ë³´
        sources = {}
        for item in news_items:
            sources[item['source']] = sources.get(item['source'], 0) + 1
        
        output.append(f"\nğŸ“Š ë‰´ìŠ¤ í†µê³„")
        output.append(f"â€¢ ì´ ë‰´ìŠ¤: {len(news_items)}ê°œ")
        output.append(f"â€¢ ì–¸ë¡ ì‚¬ë³„ ë‰´ìŠ¤:")
        for src, count in sorted(sources.items(), key=lambda x: x[1], reverse=True):
            output.append(f"  - {src}: {count}ê°œ")
        
        # ë‰´ìŠ¤ ëª©ë¡
        output.append("\nğŸ“‘ ë‰´ìŠ¤ ëª©ë¡")
        for i, item in enumerate(news_items, 1):
            published = item["published_at"].strftime("%Y-%m-%d %H:%M")
            output.append(f"\n{i}. {item['title']}")
            output.append(f"   {item['source']} | {published}")
            
            if show_summary and item["summary"]:
                summary = item["summary"]
                if len(summary) > 200:
                    summary = summary[:197] + "..."
                output.append(f"\n   {summary}")
        
        return "\n".join(output)

    def _collect_google_news(self, keyword: str, max_age_hours: int) -> List[Dict]:
        """êµ¬ê¸€ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤."""
        news_items = []
        now = datetime.now()
        
        try:
            url = self.GOOGLE_NEWS_RSS.format(query=quote_plus(keyword))
            feed = feedparser.parse(url)
            
            for entry in feed.entries:
                published_at = self._parse_datetime(entry.published)
                age_hours = (now - published_at).total_seconds() / 3600
                
                if age_hours > max_age_hours:
                    continue
                
                # ì œëª©ê³¼ ì¶œì²˜ ë¶„ë¦¬
                title_parts = entry.title.split(' - ')
                clean_title = self._clean_text(title_parts[0])
                source = title_parts[-1] if len(title_parts) > 1 else "Unknown"
                
                # ìš”ì•½ ì •ì œ
                summary = self._clean_text(entry.get("summary", ""))
                if clean_title in summary:
                    summary = summary.replace(clean_title, "").strip()
                
                news_items.append({
                    "title": clean_title,
                    "summary": summary,
                    "published_at": published_at,
                    "source": source
                })
            
        except Exception as e:
            logger.error(f"êµ¬ê¸€ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}")
        
        return news_items
    
    def _collect_naver_news(self, keyword: str, max_age_hours: int) -> List[Dict]:
        """ë„¤ì´ë²„ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤."""
        news_items = []
        now = datetime.now()
        
        try:
            url = self.NAVER_NEWS_SEARCH.format(query=quote_plus(keyword))
            response = self.session.get(url)
            soup = BeautifulSoup(response.text, 'html.parser')
            
            for news in soup.select('.news_area')[:5]:
                title_elem = news.select_one('.news_tit')
                if not title_elem:
                    continue
                
                title = self._clean_text(title_elem.get('title', ''))
                
                desc_elem = news.select_one('.dsc_txt_wrap')
                summary = self._clean_text(desc_elem.text) if desc_elem else ""
                
                source_elem = news.select_one('.info_group a:first-child')
                source = source_elem.text.strip() if source_elem else "Unknown"
                
                time_elem = news.select_one('.info_group span.info')
                published = time_elem.text.strip() if time_elem else ""
                published_at = self._parse_datetime(published)
                
                age_hours = (now - published_at).total_seconds() / 3600
                if age_hours > max_age_hours:
                    continue
                
                news_items.append({
                    "title": title,
                    "summary": summary,
                    "published_at": published_at,
                    "source": source
                })
                
        except Exception as e:
            logger.error(f"ë„¤ì´ë²„ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}")
        
        return news_items 